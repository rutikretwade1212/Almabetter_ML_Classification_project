{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "qYpmQ266Yuh3",
        "Seke61FWphqN",
        "rFu4xreNphqO",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "yiiVWRdJDDil",
        "-Kee-DAl2viO"
      ],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Health Insurance Cross sell\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Student Name-**   - Rutik Retwade\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Health Insurance Cross Sell Prediction dataset encompasses the details of 3,81,109 customers who express interest in purchasing insurance, with 10 predictor variables and one target variable. Our initial steps involved data collection, thorough data cleaning to address null values, distribution analysis, and consideration of outliers. After this, we performed typecasting to ensure that the data is in the proper format for visualization.\n",
        "\n",
        "Our analysis continued with an in-depth exploratory data analysis (EDA) phase, during which we crafted univariate, bivariate, and multivariate plots to unearth valuable insights. These insights informed our decisions regarding the subsequent steps in the machine learning (ML) model pipeline.\n",
        "\n",
        "To prepare the data for modeling, we engaged in feature engineering and tackled multicollinearity among the independent variables by employing the Variance Inflation Factor (VIF). Notably, we chose not to address outliers, as their removal could lead to a loss of significant information and potentially introduce bias into the results.\n",
        "\n",
        "We also recognized that certain features were categorical in nature and needed to be encoded into numerical values for machine learning algorithms. We accomplished this using Binary Label Encoding.\n",
        "\n",
        "The dataset presented a challenge of high class imbalance in the target variable, Response. To mitigate this, we applied the Synthetic Minority Oversampling Technique (SMOTE) to create a balanced dataset.\n",
        "\n",
        "With the data now well-prepared, we split it into train and test sets to ensure a stratified representation of both classes. Subsequently, we implemented a range of machine learning models, starting with the simple yet effective Logistic Regression, followed by Decision Trees, Random Forests, Naive Bayes, and XGBoost. We evaluated model performance using various classification metrics, including Precision, Recall, F1 Score, Accuracy, and AUC-ROC. Additionally, we assessed the model's effectiveness by examining the confusion matrix to determine the number of correctly and incorrectly classified patients."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OD5Z5WedlXPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our client, an insurance company, seeks your expertise to develop a predictive model that can anticipate whether policyholders from the previous year would express interest in the company's Vehicle Insurance offerings. Insurance policies involve a financial arrangement where the company commits to provide compensation for specified losses, damages, illnesses, or death in exchange for regular premium payments from the customers. Premiums are the recurring payments made by customers to secure these guarantees.**\n",
        "\n",
        "**For instance, a customer may pay an annual premium of Rs. 5000 for a health insurance cover of Rs. 200,000. In the event of illness and hospitalization, the insurance provider covers the hospitalization costs up to Rs. 200,000. This may seem financially challenging for the company, but it relies on the principle of probabilities. While 100 customers may pay the same premium, only a few (e.g., 2-3) may require hospitalization that year, spreading the risk among all policyholders.**\n",
        "\n",
        "**Similarly, in the context of vehicle insurance, customers pay an annual premium, and in the unfortunate event of an accident, the insurance company provides compensation, known as the 'sum assured,' to the customer.**\n",
        "\n",
        "**Creating a predictive model to determine a customer's interest in Vehicle Insurance is invaluable for the company. It enables the company to tailor its communication strategy, reach out to potential customers effectively, and optimize its business model and revenue generation.**\n",
        "\n",
        "**To make this prediction, you have access to customer information, including demographics (gender, age, region code type), vehicle details (vehicle age, damage status), and policy-related data (premium amount, sourcing channel), among other factors.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Basic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "\n",
        "# ML Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Hyper Parameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "\n",
        "# Miscellaneous\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting of drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path =('/content/drive/MyDrive/Colab Notebooks/Almabetter/ML Classification project/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')\n",
        "dataset=pd.read_csv(path)"
      ],
      "metadata": {
        "id": "RWzdny2pC5EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('Number of (rows, columns) are',dataset.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above observation we conclude that in our data set there are no duplicate values**"
      ],
      "metadata": {
        "id": "4-CpGnSKDX6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the null value in each columns\n",
        "def find_null_values(data):\n",
        "    print('Following shows the number of Missing values present in the dataset:')\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "find_null_values(dataset)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "def visualize_null_values(data):\n",
        "    print('Missing values through figure')\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    sns.heatmap(data.isnull(), cbar=True, yticklabels=False)\n",
        "    plt.xlabel(\"Column Name\", size=14, weight=\"bold\")\n",
        "    plt.title(\"Missing Values in Columns\", fontweight=\"bold\", size=17)\n",
        "    plt.show()\n",
        "\n",
        "visualize_null_values(dataset)\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The dataset 'TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION' consists of 381,109 rows and 12 columns, and it's noteworthy that no null values are present in this dataset.\n",
        "\n",
        "2. A comprehensive check for duplicates in both datasets confirms their absence.\n",
        "\n",
        "3. Furthermore, the dataset is entirely devoid of null values.\n",
        "\n",
        "4. The dataset encompasses a total of four numeric features and five categorical features, with the target variable being numeric."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. The dataset consists of **381,109 rows and 12 columns**.\n",
        "2. There are **no null values**, ensuring data completeness.\n",
        "3. **No duplicate values** are present, which supports data integrity.\n",
        "4. There are **no missing values**, making the dataset ready for analysis.\n",
        "5. The dataset contains a combination of **4 numerical features and 5 categorical features** that are essential for understanding customer responses in the health insurance industry."
      ],
      "metadata": {
        "id": "WhRMgYPeFKdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **id**: A unique identifier for each customer.\n",
        "\n",
        "2. **Gender**: The gender of the customer.\n",
        "\n",
        "3. **Age**: The age of the customer.\n",
        "\n",
        "4. **Driving_License**: A binary indicator where 0 represents a customer who does not have a driving license, and 1 represents a customer who already has a driving license.\n",
        "\n",
        "5. **Region_Code**: A unique code that identifies the region of the customer.\n",
        "\n",
        "6. **Previously_Insured**: A binary indicator where 1 represents a customer who already has vehicle insurance, and 0 represents a customer who doesn't have vehicle insurance.\n",
        "\n",
        "7. **Vehicle_Age**: The age of the customer's vehicle.\n",
        "\n",
        "8. **Vehicle_Damage**: A binary indicator where 1 represents a customer whose vehicle has been damaged in the past, and 0 represents a customer whose vehicle hasn't been damaged.\n",
        "\n",
        "9. **Annual_Premium**: The amount the customer needs to pay as a premium for the insurance policy in the year.\n",
        "\n",
        "10. **Policy_Sales_Channel**: An anonymized code that represents the channel through which the company reaches out to the customer. This could include different agents, mail, phone, in-person contact, etc.\n",
        "\n",
        "11. **Vintage**: The number of days the customer has been associated with the insurance company.\n",
        "\n",
        "12. **Response**: The target variable indicating customer interest, where 1 represents a customer who is interested, and 0 represents a customer who is not interested in the insurance product.\n",
        "\n",
        "These columns contain essential information for analyzing and predicting customer responses to the health insurance cross-sell."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print('Check Number of unique value in each column')\n",
        "print(dataset.nunique())\n",
        "print('--'*50)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in dataset.columns.tolist():\n",
        "  print(f\"The Unique Values of Variable ', {i}, 'are:\", dataset[i].unique())\n",
        "  print()\n",
        "  print('--'*50)\n",
        "  print()"
      ],
      "metadata": {
        "id": "mVgQRb_jGA3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data wrangling, indeed, plays a crucial role in the data analysis process. It involves a series of steps aimed at cleaning, transforming, and organizing raw data into a format that is suitable for analysis. This process is essential for several reasons:\n",
        "\n",
        "1. **Data Quality**: Data often contains errors, missing values, inconsistencies, and outliers. Data wrangling helps identify and rectify these issues, ensuring that the data is accurate and reliable for analysis.\n",
        "\n",
        "2. **Data Integration**: In many cases, data is collected from various sources and in different formats. Data wrangling involves merging or joining these disparate datasets to create a unified and comprehensive dataset.\n",
        "\n",
        "3. **Data Transformation**: Raw data may not be in a suitable format for analysis. Data wrangling can involve converting data types, creating new variables, and aggregating or summarizing information to make it more amenable to analysis.\n",
        "\n",
        "4. **Handling Missing Data**: Dealing with missing data is a common challenge. Data wrangling helps decide how to handle missing values, whether through imputation or exclusion, to ensure that they don't hinder analysis.\n",
        "\n",
        "5. **Data Scaling**: Scaling or normalizing data is often necessary to bring variables onto a common scale. This is crucial for certain machine learning algorithms that are sensitive to the magnitude of variables.\n",
        "\n",
        "6. **Feature Engineering**: Data wrangling can involve creating new features from existing data, which can enhance the predictive power of machine learning models.\n",
        "\n",
        "7. **Data Reduction**: In some cases, data may be too large or have too many irrelevant variables. Data wrangling can involve reducing the dimensionality of data while retaining the most critical information.\n",
        "\n",
        "8. **Data Aggregation**: Aggregating data over time periods or geographical regions can be useful for trend analysis and summarization.\n",
        "\n",
        "9. **Data Exploration**: During data wrangling, exploratory data analysis is often performed to gain insights into the data, identify patterns, and generate hypotheses for further analysis.\n",
        "\n",
        "In summary, data wrangling is a critical step in the data analysis process. It ensures that data is accurate, consistent, and in the right format for analysis, enabling more meaningful and reliable insights to be derived from the data."
      ],
      "metadata": {
        "id": "S_4KPD23GR3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Numeric of data\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "XutSNLlNGl0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Drop the ID column form the dataset\n",
        "dataset.drop(['id'] , axis=1, inplace= True)"
      ],
      "metadata": {
        "id": "mJ04yK1FGqdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to Group the Age of the customar\n",
        "def convert_numerical_to_categorical(df):\n",
        "    # Categorizing Age feature\n",
        "    df['Age_Group'] = df['Age'].apply(lambda x:'YoungAge' if x >= 20 and x<=45 else 'MiddleAge' if x>45 and x<=65 else 'OldAge')\n",
        "\n",
        "convert_numerical_to_categorical(dataset)"
      ],
      "metadata": {
        "id": "GtvzkCviGyrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "GqFaDjP4G1Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Continuous_variable =['Age', 'Region_Code', 'Annual_Premium','Policy_Sales_Channel'\t,'Vintage']"
      ],
      "metadata": {
        "id": "cEy4nLYtG3uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Display the number of Males and Females in the given dataset\n",
        "print('*' * 55)\n",
        "print('Gender Distribution in the Dataset')\n",
        "gender_counts = dataset['Gender'].value_counts()\n",
        "print(gender_counts)\n",
        "\n",
        "# Create a pie plot to visualize the distribution of Gender\n",
        "colors = ['skyblue', 'lightcoral']\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.pie(gender_counts, labels=['Male', 'Female'], colors=colors, autopct=\"%1.1f%%\", startangle=90, shadow=True)\n",
        "plt.title('Gender Distribution', fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
        "\n",
        "# Visualize the Relationship between Gender and Response\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x=\"Response\", hue=\"Gender\", palette={\"Male\": \"skyblue\", \"Female\": \"lightcoral\"}, data=dataset)\n",
        "plt.xlabel('Response', fontdict={'fontsize': 12})\n",
        "plt.ylabel('Count', fontdict={'fontsize': 14})\n",
        "plt.title('Response vs. Gender', fontdict={'fontsize': 15, 'fontweight': 'bold'})\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've used a pie chart to show the breakdown of percentages for our dependent variable, which is great for comparing different percentages within a circle.\n",
        "\n",
        "For exploring relationships between two variables, we've used a catplot. It's a helpful choice to understand how these variables interact in a straightforward way."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pie chart, it's evident that there are 206,089 individuals, accounting for 54.1% of the total, in the Male category, and 175,020 individuals, which makes up 45.9%, in the Female category within the dataset. This observation indicates that the number of individuals in the Male category is notably higher compared to the Female category."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our analysis of the graph suggests that more men are interested in buying health insurance compared to women. So, it's a good idea for the company to pay extra attention to men, as they seem to be more interested in our health insurance offerings."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2: Visualizations\n",
        "\n",
        "# Pie chart for the distribution of age groups\n",
        "plt.figure(figsize=(8, 8))\n",
        "dataset['Age_Group'].value_counts().plot(kind='pie', explode=(0.1, 0.1, 0.1), shadow=True, autopct='%1.2f%%', pctdistance=1.5, labeldistance=1.8)\n",
        "plt.title('Age Group Distribution', fontsize=15, fontweight='bold')\n",
        "\n",
        "# Catplot to explore the relationship between Response and Age Group\n",
        "sns.catplot(x=\"Response\", hue=\"Age_Group\", kind=\"count\", palette=\"pastel\", data=dataset)\n",
        "plt.xlabel('Response', fontsize=15)\n",
        "plt.ylabel('Count', fontsize=15)\n",
        "plt.title('Response vs. Age Group', fontsize=15, fontweight='bold')\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a pie chart to show how parts make up the whole, making it easy to compare percentages using different colors. When we want to compare different percentages, a pie chart is the way to go.\n",
        "\n",
        "For understanding how two things relate to each other, we used a catplot. Catplots are handy for seeing how variables work together. So, a pie chart helps with percentages, and a catplot helps with understanding how things are connected."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart shows that:\n",
        "\n",
        "- About 67.53% of the data is from the young age group.\n",
        "- Middle-aged people make up around 25.11% of the data.\n",
        "- The old-age group accounts for roughly 7.37% of the data.\n",
        "\n",
        "What's notable is that a larger number of responses come from the young age group, suggesting they are more interested in our offerings."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart and count plot show that:\n",
        "\n",
        "- Younger people are quite interested in buying health insurance, which is great for our business.\n",
        "- Middle-aged individuals are somewhat less interested compared to the younger age group.\n",
        "- Response from older people is very low.\n",
        "\n",
        "In simple terms, concentrating on the younger age group can boost our business, as they are showing the most interest."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "print('*'*55)\n",
        "print('How many of them have Driving Liscense')\n",
        "print(dataset['Driving_License'].value_counts())\n",
        "\n",
        "\n",
        "# Distribution of Driving License through pieplot\n",
        "print('*'*55)\n",
        "dataset['Driving_License'].value_counts().plot(kind='pie', figsize=(15,6), autopct=\"%1.1f%%\", startangle=90,shadow=True,\n",
        "                               labels=['Having Driving Liscense(%)','Not Having Driving Liscense(%)'],\n",
        "                               colors=['Brown','green'], explode=[0,0])"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "sns.barplot(x='Driving_License', y='Response', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JObqcRG2Knqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pick up the pie chart and bar graph wher pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable and Bar graph gives the relationship between two Driving Liscense."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart we ahve found that 99.8% of the data have Driving License and onle 0.2% and high volume of data have response Yes those have Driving License."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the pieplot that the data which having Driving License can make good impect in business and those do not have driving Liscense not useful for us so we have to targe data having Driving Liscense."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.countplot(x='Region_Code',hue='Response',data=dataset)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A count plot is like a bar chart that helps us compare different groups in our data. It shows which groups are most common and how they relate to each other. In our case, we're using it to see how different reasons are connected to responses."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Dependant Column Value Counts\n",
        "print('*'*55)\n",
        "print('How many of them have  Previously Insurenced')\n",
        "print(dataset['Previously_Insured'].value_counts())\n",
        "\n",
        "# Dependant Variable Column Visualization\n",
        "print('*'*50)\n",
        "dataset['Previously_Insured'].value_counts().plot(kind='pie', figsize=(15,6), autopct=\"%1.1f%%\", startangle=180, shadow=True,\n",
        "                               labels=['Do not have Previously_Insured(%)','Previously_Insured(%)'],\n",
        "                               colors=['purple','green'],explode=[0,0])"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x='Previously_Insured', y='Response', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rIV_JQ0RLble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart we have found that 54.2% of data belong the Do not have Previously_Insurece and 45.8% of them are have Previously_Insurence from the dataset."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Display the value counts for the 'Vehicle_Damage' column\n",
        "print(\"Count of 'Vehicle_Damage' values:\")\n",
        "print(dataset['Vehicle_Damage'].value_counts())\n",
        "\n",
        "# Create a pie chart to visualize the distribution of 'Vehicle_Damage'\n",
        "colors = ['purple', 'green']\n",
        "plt.figure(figsize=(15, 6))\n",
        "dataset['Vehicle_Damage'].value_counts().plot(kind='pie', autopct=\"%1.1f%%\", startangle=180, shadow=True, labels=['Vehicle has Damage', 'Vehicle has no damage'], colors=colors, explode=[0, 0])\n",
        "plt.title('Vehicle Damage Distribution')\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Visualization\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.barplot(x='Vehicle_Damage', y='Response', data=dataset, palette='muted')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5ksSohnCL-AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbvrpDEBMUWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've chosen to use a pie chart to illustrate the part-to-whole relationship in our data. A pie chart makes it simple to understand percentage comparisons by dividing a circle into portions with different colors. Whenever we need to compare different percentages, a pie chart is a common and useful choice. In our case, it helped us visually compare the percentages of the dependent variable."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart above provides these insights:\n",
        "\n",
        "- Approximately 50.5% of the data belongs to customers with vehicles that have damage.\n",
        "- About 49.5% of the data is from customers whose vehicles do not have damage.\n",
        "\n",
        "Additionally, the pie chart suggests that customers who already have a driving license are more likely to be interested in purchasing health insurance."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Vehicle Age Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Vehicle_Age', hue='Response', data=dataset, palette='Set2')\n",
        "plt.xlabel('Count', fontsize=14)\n",
        "plt.ylabel('Vehicle Age', fontsize=14)\n",
        "plt.title('Vehicle Age Distribution', fontsize=15, fontweight='bold')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pick up countplot chart which shows the relationship between Vehicle age and response which helps us to analyse the dataset."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here's a simplified version of your observations:\n",
        "\n",
        "- Customers with vehicle age less than 1 year have a very low chance of purchasing insurance.\n",
        "- Customers with vehicle age between 1 to 2 years are more likely to be interested in insurance compared to the other two categories.\n",
        "- Customers with vehicle age less than 1 year have a very low chance of buying insurance.\n",
        "\n",
        "These observations emphasize the significance of vehicle age in determining customer interest in insurance."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the given data we found that those data which have Vehicle_Age between 1-2 Year are highly impect in our business the the chances of the customer to chouse the Health Insurence."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for each column to know the data distribution\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "def Distribution_plot(data):\n",
        "  for col in Continuous_variable:\n",
        "    fig=plt.figure(figsize=(9,6))\n",
        "    ax=fig.gca()\n",
        "    feature= (data[col])\n",
        "    sns.distplot(data[col])\n",
        "    ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "    ax.set_title(col)\n",
        "  plt.show()\n",
        "\n",
        "Distribution_plot(dataset)"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used distplot of find the univariate distribution of the continuous variable of the dataset against the density distribution."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chouse the histplot to find the univariate analysis of the dataset and find mean, median of the distributed data of all the variable in the dataset."
      ],
      "metadata": {
        "id": "GaLQlxjxUOUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(dataset.corr(), cmap ='PiYG', annot = True)"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Answer.** We have pick up this heatmap chart to find insights to analyse that how the given one variable are the corelation to another variable."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "From the heatmap, we can see that there is no significant correlation between any variables. Therefore, we can proceed with implementing algorithms without the need to address correlations.\n",
        "\n",
        "Variables with Negative Correlation:\n",
        "- Previously_Insured\n",
        "- Policy_Sales_Channel\n",
        "- Vintage\n",
        "\n",
        "Variables with Positive Correlation:\n",
        "- Age\n",
        "- Driving License\n",
        "- Region Code\n",
        "- Annual Premium\n",
        "\n",
        "This information helps us understand the relationships between the variables in the dataset.\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Pair Plot visualization code\n",
        "sns.pairplot(dataset,hue='Response')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is like a visual guide that helps us figure out which features work best together to explain relationships between variables or create distinct groups. It's a way to see patterns in the data and how features relate to each other. Think of it as a visual map of these relationships, which can be handy for understanding the data better."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart above tells us that there is not a strong linear relationship between the variables, and the data points are not easily separated by simple straight lines. This means that the data doesn't show clear linear patterns, and it might require more complex models or approaches to analyze and understand."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have different statistical tests for different scenarios:\n",
        "1. Single categorical feature -> One proportion test\n",
        "2. Two categorical features -> Chi squared test\n",
        "3. More than two category in categorical features -> ANOVA test\n",
        "4. One numerical and one categorical(=2 categories) feature-> ANOVA test\n",
        "5. One numerical feature -> T-test\n",
        "6. Two numerical feature -> Corelation test\n",
        "7. One numerical and one categorical(>2 categories) feature -> T-test\n",
        "\n",
        "Let's just define three hypothetical statements and perform the needed tests for the same\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis :** There is relation between \"Age\" of the customer  their \"Response\" to get Health Insurence.\n",
        "\n",
        "**Alternate Hypothesis :** There is a no relationship between \"Age\" and their \"Response:\" to get Insurence."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# defining the table\n",
        "data = [dataset['Age'], dataset['Response']]\n",
        "stat, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "# interpret p-value\n",
        "alpha = 0.05\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "    print('Reject null hypothesis')\n",
        "else:\n",
        "    print('Accept null hypothesis')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi Square Test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used chi-square test in order to determine whether there is a significant association between the two variables. In our case 'Age' and 'Responce' are the two variables. test shows that age and Response have a significant impact on each other,therefore we Accept null hypothesis"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis :** There is no relation between \"Temperature\" and \"Ranted Bike Coount\"\n",
        "\n",
        "**Alternate Hypothesis :** There is a relationship between \"Temperature\" and \"Ranted Bike Coount\""
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "\n",
        "first_sample1 = dataset[\"Driving_License\"].head(200000)\n",
        "second_sample1 = dataset[\"Response\"].head(200000)\n",
        "\n",
        "stat, p = pearsonr(first_sample1, second_sample1)\n",
        "print('stat=%.3f, p = %.5f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Pearson Correlation test in order to determine whether there is a significant association between the two variables. In our case 'Driving License' and 'Responce' are the two variables. test shows that 'Driving LIcense' and 'Response' have not significant impact on each other,therefore we Rejected Null Hypothesis."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjuUKYfqbQ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis :** There is no relation between \"Vintage\" and \"Response\"\n",
        "\n",
        "**Alternate Hypothesis :** There is a relationship between \"Vintage\" and \"Response\""
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# defining the table\n",
        "data = [dataset['Vintage'], dataset['Response']]\n",
        "stat, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "# interpret p-value\n",
        "alpha = 0.05\n",
        "print(\"p value is \" + str(p))\n",
        "if p <= alpha:\n",
        "    print('Reject null hypothesis')\n",
        "else:\n",
        "    print('Accept null hypothesis')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi Square Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used  Pearson Correlation test in order to determine whether there is a significant association between the two variables. In our case 'Vintage' and 'Responce' are the two variables. test shows that 'Vintage' and 'Response' have not significant impact on each other,therefore we Rejected Null Hypothesis."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is no missing values persent in the dataset so there is no need to heldle missing values from this dataset.**"
      ],
      "metadata": {
        "id": "OLTzyVjDbfW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are working with large set of data then there is chance that missing value persent into it so we have to handle error and following are the techenique to handle missing value--\n",
        "\n",
        "\n",
        "\n",
        "1.   Deleting Rows with missing values\n",
        "\n",
        "2.   Impute missing values for continuous variable\n",
        "\n",
        "3.   Using Algorithms that support missing values\n",
        "\n",
        "4.   Prediction of missing values\n",
        "\n",
        "5.   Imputation using Deep Learning Library\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# defining the code for outlier detection and percentage using IQR.\n",
        "def find_outliers(data):\n",
        "    outliers = []\n",
        "    data = sorted(data)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q2 = np.percentile(data, 50)\n",
        "    q3 = np.percentile(data, 75)\n",
        "    print(f\"q1:{q1}, q2:{q2}, q3:{q3}\")\n",
        "\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    print(f\"Lower bound: {lwr_bound}, Upper bound: {upr_bound}, IQR: {IQR}\")\n",
        "\n",
        "    for i in data:\n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    len_outliers= len(outliers)\n",
        "    print(f\"Total number of outliers are: {len_outliers}\")\n",
        "\n",
        "    print(f\"Total percentage of outlier is: {round(len_outliers*100/len(data),2)} %\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to find outliers\n",
        "def showoutliers(data):\n",
        "  plt.figure(figsize=(30,15))\n",
        "  for n,column in enumerate(data.describe().columns):\n",
        "    plt.subplot(5, 4, n+1)\n",
        "    sns.boxplot(data[column])\n",
        "    plt.title(f'{column.title()}',weight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "showoutliers(dataset)"
      ],
      "metadata": {
        "id": "7TFDQnikcIJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define variable\n",
        "continuous_variable = ['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel','Age']\n",
        "categorical_variable = [ 'Driving_License','Previously_Insured','Vintage', 'Response']\n",
        "object_data= ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Age_Group']"
      ],
      "metadata": {
        "id": "3pFHJVoacLoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining IQR, Lower and Upper bound and number out outliers present in each of the continous numerical feature\n",
        "for feature in continuous_variable:\n",
        "  print('--'*50)\n",
        "  print('** Percentage of outliers of continuous variable of columns **')\n",
        "  print(feature,\":\")\n",
        "  find_outliers(dataset[feature])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "8MVnPHSjcO1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Defining the function that treats outliers with the IQR technique\n",
        "def treat_outliers_iqr(data):\n",
        "    # Calculate the first and third quartiles\n",
        "    q1, q3 = np.percentile(data, [25, 75])\n",
        "\n",
        "    # Calculate the interquartile range (IQR)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Identify the outliers\n",
        "    lower_bound = q1 - (1.5 * iqr)\n",
        "    upper_bound = q3 + (1.5 * iqr)\n",
        "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "\n",
        "    # Treat the outliers (e.g., replace with the nearest quartile value)\n",
        "    treated_data = [q1 if x < lower_bound else q3 if x > upper_bound else x for x in data]\n",
        "    treated_data_int = [int(absolute) for absolute in treated_data]\n",
        "\n",
        "    return treated_data_int"
      ],
      "metadata": {
        "id": "0OIzDe5YcRFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing all the feature one by one from the list of continous_value_feature in our above defined function for outlier treatment\n",
        "for feature in continuous_variable:\n",
        "  dataset[feature]= treat_outliers_iqr(dataset[feature])\n",
        "\n",
        "\n",
        "# Determining IQR, Lower and Upper bound and number out outliers present in each of the continous numerical feature a\n",
        "for feature in continuous_variable:\n",
        "  print('--'*50)\n",
        "  print('** After treating outlierrs the percentage of outliers are : **')\n",
        "  print(feature,\":\")\n",
        "  find_outliers(dataset[feature])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "KZTZF1lWcTRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,15))\n",
        "for n,column in enumerate(dataset.describe().columns):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(dataset[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "hDa0obEUcVB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Create copy of the dataset\n",
        "df = dataset.copy()\n",
        "#Drop the Age column because we have created Age_group columns\n",
        "df.drop('Age', axis=1, inplace= True)\n",
        "\n",
        "# List of variable that have to make label encoding\n",
        "col =['Gender', 'Vehicle_Damage', 'Vehicle_Age', 'Age_Group']\n",
        "\n",
        "#Import library for label Encoding\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for label Encoding\n",
        "def Label_encoding(data):\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  for var in col:\n",
        "    data[var] = label_encoder.fit_transform(data[var])\n",
        "\n",
        "Label_encoding(df)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-b9fbz2Rci2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've utilized Label Encoding to transform the 'Gender', 'Vehicle_Damage', 'Vehicle_Age', and 'Age_Group' columns into a numerical format. This technique is beneficial for enhancing the performance of machine learning models, as it allows the model to understand and work with categorical variables by converting them into numerical representations. In essence, Label Encoding simplifies the data and makes it suitable for machine learning algorithms, contributing to better model performance."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(), cmap ='PiYG', annot = True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's include only those features in our final dataframe that are highly impacting on the dependent variable. For this we are using Variance Inflation Factor technique to determine multicolinearity"
      ],
      "metadata": {
        "id": "JkSd87B2cz8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Import VIF library\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Code to calculate VIF\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns ]])"
      ],
      "metadata": {
        "id": "4O3aQo6kc7H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating VIF(Variance Inflation Factor) by excluding :\n",
        "\n",
        "Driving_license variable because It give 30.00 VIF which is very high that why we not include it."
      ],
      "metadata": {
        "id": "veDr9d6Cc9LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the vif by excluding some features which are not giving any information\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Driving_License']]])"
      ],
      "metadata": {
        "id": "76pQ4tGTdCdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = df[[i for i in df.describe().columns if i not in ['Driving_License']]]"
      ],
      "metadata": {
        "id": "cd6quFn7dEQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have plotted the seaborn's scatterplot and seaborn's heatmap to see the relationship of each of the feature with target variable and observed that some features like BPmeds, diabetes, totchol etc. are positively correlated with target variable. While sex, education are negatively correlated with target variable."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.columns"
      ],
      "metadata": {
        "id": "HUXP0TVMdLGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have selected these feature and we found that all the feature does not have\n",
        "high VIF which impect our dataset. the feature which is important for our dataset which is stored in final_df"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x = final_df.drop(columns='Response',axis =1)\n",
        "y= final_df['Response']\n",
        "\n",
        "## Importing train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Spliting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of in training dataset\n",
        "print(\"Distribution of data for dependent variable in train :\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Checking the distribution of in test dataset\n",
        "print(\"Distribution of data for dependent variable in test :\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "d2P5XWZ0dgsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have distributed the training and test dataset in the ratio of 80% (Training) and 20% (Test) from the whole dataset."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is highly imbalance, which is obvious. The number of customer who is intrested to take Helth Insurense the response of  number 0 and 1 show that the customer is intrested to Health Insurense or not so let's check the dataset."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "print('Check the given data is balanced or not')\n",
        "\n",
        "print('--'*50)\n",
        "plt.title(\"Response class Distribution\")\n",
        "sns.countplot(df['Response'])\n",
        "plt.title(\"Transaction Class Distribution\")\n",
        "df['Response'].value_counts()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M2DlsYyhd0U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing SMOTE for balancing the dataset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Fitting the data\n",
        "smote = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "x_sm, y_sm = smote.fit_resample(x, y)\n",
        "\n",
        "# Checking Value counts for both classes Before and After handling Class Imbalance:\n",
        "for col,label in [[y,\"Before\"],[y_sm,'After']]:\n",
        "  print(label+' Handling Class Imbalace:')\n",
        "  print(col.value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "fPMMTbQwdvsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are using SMOTE on the training set to address these imbalances. The Synthetic Minority Oversampling Technique, or SMOTE for short, is a type of data augmentation for the minority class. The strategy works because it generates convincing new synthetic examples from the minority class that are substantially near in feature space to already existing examples from the minority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Respliting the dataset after using SMOTE\n",
        "x_smote_train, x_smote_test, y_smote_train, y_smote_test = train_test_split(x_sm,y_sm , test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "WA4WwoyryJoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of in train dataset  dependent Variable\n",
        "print(\"Distribution of data for dependent variable in test :\")\n",
        "print(y_smote_train.value_counts())\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Check the distribution of in test dataset  dependent Variable\n",
        "print(\"Distribution of data for dependent variable in test :\")\n",
        "print(y_smote_train.value_counts())"
      ],
      "metadata": {
        "id": "g55QUFKWyMZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Rescaling your data\n",
        "# Importing StandardScaler for Data Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Creating object\n",
        "std_scaler= StandardScaler()\n",
        "\n",
        "# Fit and Transform\n",
        "x_smote_train= std_scaler.fit_transform(x_smote_train)\n",
        "x_smote_test= std_scaler.transform(x_smote_test)"
      ],
      "metadata": {
        "id": "1BwTliAIi2by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QzXHH4qSgpuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1) Logestic regression**"
      ],
      "metadata": {
        "id": "Bs6FDB6Ugwb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train the input model and print evaluation matrics such as classification report, confusion matrix and AUC-ROC curve in visualize format\n",
        "def analyse_model(model, x_train, x_test, y_train, y_test):\n",
        "\n",
        "  '''Takes classifier model, train-set and test-set as input and prints the evaluation matrices in visualize format and returns the model'''\n",
        "\n",
        "  # Fitting the model\n",
        "  model.fit(x_train,y_train)\n",
        "\n",
        "  # Finding best parameters\n",
        "  try:\n",
        "    print(f\"The best parameters are: {model.best_params_}\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Plotting Evaluation Metrics for train and test dataset\n",
        "  for x, act, label in ((x_train, y_train, 'Train-Set'),(x_test, y_test, \"Test-Set\")):\n",
        "\n",
        "    # Getting required metrics\n",
        "    pred = model.predict(x)\n",
        "    pred_proba = model.predict_proba(x)[:,1]\n",
        "    report = pd.DataFrame(classification_report(y_pred=pred, y_true=act, output_dict=True))\n",
        "    fpr, tpr, thresholds = roc_curve(act, pred_proba)\n",
        "\n",
        "    # Classification report\n",
        "    plt.figure(figsize=(18,3))\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.heatmap(report.iloc[:-1, :-2].T, annot=True, cmap=sns.color_palette(\"crest\", as_cmap=True),fmt=\".2f\",annot_kws={\"fontsize\":14, \"fontweight\":\"bold\"},linewidths=1.0)\n",
        "    plt.title(f'{label} Classification Report')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.subplot(1,3,2)\n",
        "    matrix= confusion_matrix(y_true=act, y_pred=pred)\n",
        "    sns.heatmap(matrix, annot=True, cmap=sns.color_palette(\"flare\", as_cmap=True),fmt=\".2f\", annot_kws={\"fontsize\":14, \"fontweight\":\"bold\"},linewidths=1.0)\n",
        "    plt.title(f'{label} Confusion Matrix')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('Actual labels')\n",
        "\n",
        "    # AUC_ROC Curve\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.plot(fpr,tpr,label=f'AUC = {np.round(np.trapz(tpr,fpr),3)}')\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(f'{label} AUC_ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing LogisticRegression from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fitting Logistic Regression Model and Visualizing evaluation Metric Score chart\n",
        "logistic_classifier = LogisticRegression(fit_intercept=True, penalty='l2',max_iter=20000,random_state=0)\n",
        "\n",
        "# Predict on the model\n",
        "analyse_model(logistic_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying Logistic Regression, which is a relatively simple binary classification model, we obtained the following performance metrics:\n",
        "\n",
        "- Accuracy: 79%\n",
        "- Precision: 73% for class 1 and 89% for class 0\n",
        "- Recall: 92% for class 1 and 66% for class 0\n",
        "- F1 score: 81% for class 1 and 76% for class 0\n",
        "- AUC-ROC curve: 85.3% for the training dataset and 85.4% for the testing dataset.\n",
        "\n",
        "These metrics provide an evaluation of the model's performance in predicting customer interest in buying health insurance. The model shows good accuracy and a balanced trade-off between precision and recall."
      ],
      "metadata": {
        "id": "vG6y_FsVtBs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Decision tree"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Import Decission Tree algorithms\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Fitting Decission tree algorthms\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "# Analyse the model\n",
        "analyse_model(clf_gini,x_smote_train,x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implemented Logistic Regression we have proceded with Decission Tree Algorithms we found that there is slightly increase in the accuracy from 79% to 80%\n",
        "\n",
        "--> Accuracy of 80%\n",
        "\n",
        "--> Precision of 73% fror class1 and 93% for class0\n",
        "\n",
        "--> Recall of 95% for class1 and 65% for class0,\n",
        "\n",
        "--> F1 score 83% for class1 and 77% for class0\n",
        "\n",
        "--> AUC_ROC curve 84.9% for Train 85% for Test"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 3 Random Forest**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fitting RandomForestClassifier Model\n",
        "RF_classifier = RandomForestClassifier(n_estimators=500,max_depth=3,n_jobs=-1,random_state=0)\n",
        "\n",
        "# Analysing the model and Visualizing evaluation Metric Score chart\n",
        "analyse_model(RF_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implemented Logistic Regression and Decission Tree next we have implemented Random Forest Algorithms we found that there is slightly  increase in their accuracy of 80% to 81%.\n",
        "\n",
        "--> Accuracy of 81%\n",
        "\n",
        "--> Precision of 75% fror class 1 and 90% for class 0.\n",
        "\n",
        "--> Recall of 93% for class 1 and 69% for class 0.\n",
        "\n",
        "--> F1 score 83% for class 1 and 78% for class 0.\n",
        "\n",
        "--> AUC_ROC curve 86.8% for Train and Test both."
      ],
      "metadata": {
        "id": "6_1eYIfrnELV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applyed Logistic regression we have implemented Decission Tree Classification algorithms we have achived recall nof .95 on train and .95 on test dataset alonge with auc-roc score 84.9% and accuracy 85%"
      ],
      "metadata": {
        "id": "g9-BAeNgnHmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ML model 3- Random Forest Classifier**"
      ],
      "metadata": {
        "id": "uG4t2CcStfZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fitting RandomForestClassifier Model\n",
        "RF_classifier = RandomForestClassifier(n_estimators=500,max_depth=3,n_jobs=-1,random_state=0)\n",
        "\n",
        "# Analysing the model and Visualizing evaluation Metric Score chart\n",
        "analyse_model(RF_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "FvaqtZp2toHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing Random Forest, we observed a slight increase in accuracy compared to Logistic Regression:\n",
        "\n",
        "- Accuracy: 81%\n",
        "- Precision: 75% for class 1 and 90% for class 0\n",
        "- Recall: 93% for class 1 and 69% for class 0\n",
        "- F1 score: 83% for class 1 and 78% for class 0\n",
        "- AUC-ROC curve: 86.8% for both the training and testing datasets.\n",
        "\n",
        "Random Forest shows improved performance, with a higher accuracy and better precision and recall for class 1. It also has a higher AUC-ROC score, indicating its effectiveness in predicting customer interest in health insurance."
      ],
      "metadata": {
        "id": "RP1BRNCXtzP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying Decision Tree Classification, we achieved the following results:\n",
        "\n",
        "- Recall: 0.95 on both the training and testing datasets.\n",
        "- AUC-ROC score: 84.9%.\n",
        "- Accuracy: 85%.\n",
        "\n",
        "Decision Tree Classification showed an excellent recall rate of 95% on both the training and testing datasets, which means it is effective at identifying customers interested in health insurance. The AUC-ROC score of 84.9% and an accuracy of 85% indicate its overall good performance."
      ],
      "metadata": {
        "id": "Z207ym3Yt9kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing GridSearchCV from sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Defining classifier instance\n",
        "classifier= RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Defining parameters\n",
        "grid_values = {'n_estimators':[150,250,300,350], 'max_depth':[7,8,10]}\n",
        "\n",
        "# Fitting RandomForestClassifier Model with GridSearchCV\n",
        "RF_grid_classifier = GridSearchCV(classifier, param_grid = grid_values, scoring = 'roc_auc', cv=3)\n",
        "\n",
        "# Analysing the model\n",
        "analyse_model(RF_grid_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to minimise the errors between actual and predicted values, we evaluate our ML model using different metrics such as Recall, F-1 score, Accuracy and AUC-ROC. All these metrics try to give us an indication on how close we are with the real/expected output. In our case, each evaluation metric is showing not much difference on the train and test data which shows that our model is predicting a closer expected value"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ML Model - Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "ZLH1Q6r0uKyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Fitting model\n",
        "NB_classifier =GaussianNB()\n",
        "\n",
        "# Analyse the model\n",
        "analyse_model(NB_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "wFy0TXwDuQ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing the Naive Bayes algorithm, we obtained the following results:\n",
        "\n",
        "- Accuracy: 78%\n",
        "- Precision: 70% for class 1 and 94% for class 0.\n",
        "- Recall: 96% for class 1 and 59% for class 0.\n",
        "- F1 score: 81% for class 1 and 72% for class 0.\n",
        "- AUC-ROC curve: 84.2% for the training dataset and 84.1% for the testing dataset.\n",
        "\n",
        "Naive Bayes achieved an accuracy of 78%, which is lower compared to other models. However, it showed a high recall rate of 96% for identifying customers interested in health insurance. The precision for class 0 is relatively high at 94%, indicating that the model is good at correctly classifying customers not interested in health insurance. The AUC-ROC curve scores indicate a reasonable level of performance."
      ],
      "metadata": {
        "id": "Q61KZC6lvBDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ML Model XG boost**"
      ],
      "metadata": {
        "id": "n1rW8j6xvQmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Fitting XGBClassifier Model\n",
        "XGB_classifier = XGBClassifier(n_estimators=150,max_depth=3,n_jobs=-1,random_state=0)\n",
        "\n",
        "# Analysing the model and Visualizing evaluation Metric Score chart\n",
        "analyse_model(XGB_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "_tpcVyVPvYqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing GridSearchCV from sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Defining classifier instance\n",
        "classifier= XGBClassifier(random_state=0)\n",
        "\n",
        "# Defining parameters\n",
        "grid_values = {'learning_rate':[0.01, 0.1,1],'n_estimators':[250,300,350], 'max_depth':[2,3,4]}\n",
        "\n",
        "# Fitting RandomForestClassifier Model with GridSearchCV\n",
        "XGB_grid_classifier = GridSearchCV(classifier, param_grid = grid_values, scoring = 'roc_auc', cv=3)\n",
        "\n",
        "# Analysing the model\n",
        "analyse_model(XGB_grid_classifier, x_smote_train, x_smote_test, y_smote_train, y_smote_test)"
      ],
      "metadata": {
        "id": "v1Ife9AqvdCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing different regression metrics in order to make dataframe and compare them\n",
        "models = [\"Logistic_regression\",\"Decision Tree\",'Random Forest', 'Naive Bayes',\"XGboost\"]\n",
        "Accuracy =  [0.79,  0.80, 0.81, 0.78, 0.84]\n",
        "Precision = [0.73,  0.73, 0.77, 0.70, 0.81]\n",
        "Recall =    [0.92,  0.95, 0.93, 0.96, 0.90]\n",
        "F1_Score=   [0.81,  0.83, 0.84, 0.81, 0.85]\n",
        "AUCROC =    [0.85,  0.86, 0.90, 0.84, 0.93]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'Models': models,\n",
        "        'Accuracy' : Accuracy,\n",
        "        'Precision': Precision,\n",
        "        'Recall': Recall,\n",
        "        'F1_Score': F1_Score,\n",
        "        'AUCROC': AUCROC,\n",
        "       }\n",
        "metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "metric_df"
      ],
      "metadata": {
        "id": "I68WbCT-nfJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have choosen XG-Boost Model because it's gives **93.7% for train and 92.8 test AUC_ROC** and Highest **accuracy of 86% for train and 84% for test** and the difference between class 0 and class 1  is minimum as compare to orther model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion from EDA**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the key insights from the dataset:\n",
        "\n",
        "1. In the dataset, 54.1% of the customers are male, while 45.9% are female. Male customers show a higher interest in buying health insurance.\n",
        "\n",
        "2. Approximately 67.53% of the customers fall into the age group of 20 to 45, 25.11% belong to the age group of 45 to 65, and 7.37% are above the age of 65.\n",
        "\n",
        "3. Nearly 99.8% of the customers in the dataset have a valid driving license, while only 0.2% do not possess a driving license.\n",
        "\n",
        "4. About 45.8% of the customers in the dataset already have insurance coverage.\n",
        "\n",
        "These insights provide a better understanding of the customer demographics and their response to health insurance."
      ],
      "metadata": {
        "id": "-Qqvnlfjnx6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion from Machine Learning Model**"
      ],
      "metadata": {
        "id": "UNNRINBAn1np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We applied five different classification machine learning models to analyze the data, including Logistic Regression, Decision Tree, Random Forest, Naive Bayes, and XG Boost.\n",
        "\n",
        "Out of these models, XG Boost outperformed the others, achieving the highest recall, precision, F1 score, accuracy, and AUC-ROC score. This indicates that XG Boost is the most effective model for this dataset and can provide the best predictions for customer responses."
      ],
      "metadata": {
        "id": "EFSdox_-n8oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29r1pVyWyVCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 5]\n",
        "new_list = [x * 2 for x in my_list if x % 2 == 0]\n",
        "print(new_list)"
      ],
      "metadata": {
        "id": "EPrHjFSxDUTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-d2QXqhckN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}